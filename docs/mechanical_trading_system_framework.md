# Mechanical Trading System Framework

**Document Purpose**: This document serves as a comprehensive guide for understanding, developing, and maintaining the mechanical trading system. It outlines the system's architecture, components, data flow, and phased implementation plan. It is intended for developers, especially those new to the project, to quickly grasp the system's design and how to contribute.

## System Overview

The system is designed to automatically ingest live market data, process this data to identify trading signals based on predefined strategies across multiple timeframes, and (eventually) coordinate these signals to make trading decisions. It prioritizes robustness, data integrity, and modularity.

## Core Components:

1.  **Live Data Ingestion (`src/data/ingestion/live_ingester.py`)**:
    *   **Purpose**: Captures real-time market data (trades and quotes) from the ProjectX Gateway via a SignalR WebSocket connection.
    *   **Functionality**: Handles authentication (JWT), subscription to specific financial instruments (contracts), and robust message parsing.
    *   **Key Technology**: `signalrcore` library for Python.

2.  **OHLC Aggregation (within `live_ingester.py` and `OHLCBarAggregator` class)**:
    *   **Purpose**: Converts a stream of raw tick data (individual trades) into standardized Open, High, Low, Close (OHLC) bars for various user-defined timeframes (e.g., 1 minute, 5 minutes, 1 hour, 4 hours, 1 day).
    *   **Functionality**: Manages pending bars for each contract and timeframe, updates bar values with each new tick, and finalizes bars upon period completion. Ensures timestamps are correctly aligned to standard clock intervals.
    *   **Importance**: OHLC bars are a fundamental data structure for most technical analysis strategies.

3.  **TimescaleDB Storage (`src/data/storage/db_handler.py`, `scripts/setup_local_db.py`)**:
    *   **Purpose**: Persistently stores the generated OHLC bars and other system data (like signals and watermarks) in an efficient and queryable manner.
    *   **Key Technology**: TimescaleDB (a PostgreSQL extension optimized for time-series data). Chosen for its performance with financial data, hypertable capabilities (automatic partitioning by time), and SQL interface.
    *   **Tables**:
        *   `ohlc_bars`: Stores OHLCV data.
        *   `detected_signals`: Stores signals generated by analyzers.
        *   `analyzer_watermarks`: Tracks the last processed data point for each analyzer to prevent reprocessing.
        *   `coordinator_watermarks`: (Future use) Tracks the last processed signal for the coordinator.

4.  **Multi-Timeframe Signal Engine (`src/analysis/analyzer_service.py`, `src/strategies/trend_start_finder.py`)**:
    *   **Purpose**: Analyzes OHLC data from the database for different contracts and timeframes to identify potential trading signals based on implemented strategies (currently CUS/CDS trend detection).
    *   **Functionality**: Periodically queries new OHLC data (using watermarks), applies strategy logic, and stores identified signals in the `detected_signals` table. (See Phase 2 for proposed event-driven enhancements).
    *   **Strategy**: The current primary strategy (`trend_start_finder.py`) is based on identifying "Confirmed Uptrend Starts" (CUS) and "Confirmed Downtrend Starts" (CDS) using a specific set of bar patterns and state transitions.

5.  **Signal Coordination & Execution Logic (Future Development)**:
    *   **Purpose**: To combine signals from different analyzers (potentially across multiple timeframes or strategies) into a single, actionable trading decision. This component will then interact with an execution module to place orders.
    *   **Functionality (Conceptual)**: Will involve rules for signal confluence, filtering, and risk management before triggering an execution.

6.  **Trend Start Data API (`web/src/app/api/trend-starts/route.ts`)**:
    *   **Purpose**: Provides an HTTP API endpoint to retrieve stored trend start signals.
    *   **Functionality**: Allows querying signals based on timeframe, contract ID, signal type, date range, and pagination.
    *   **Key Technology**: Next.js API Routes, Prisma ORM for database interaction.

## Guiding Principles:

*   **Event-Driven**: The system primarily reacts to new data (ticks from the market, new bars in the database) rather than constantly polling for changes. This is efficient and responsive. The transition of `analyzer_service.py` to an event-driven model using LISTEN/NOTIFY further emphasizes this.
*   **Asynchronous Operations**: Utilizes `asyncio` for network I/O (SignalR) and potentially for other I/O-bound tasks to prevent blocking the main execution threads, ensuring the system remains responsive.
*   **Data Integrity**: Critical for a trading system. Achieved through:
    *   Unique constraints in the database (e.g., on `ohlc_bars` for contract, timestamp, and timeframe) to prevent duplicates.
    *   Careful timestamping and alignment of OHLC bars.
    *   Watermarking to ensure no data is missed or reprocessed unnecessarily.
*   **Modularity**: Components (ingestion, aggregation, analysis, storage) are designed to be as independent as possible. This simplifies development, testing, and maintenance. For example, the signal generation logic in `trend_start_finder.py` is separate from the `analyzer_service.py` that orchestrates its execution.
*   **Scalability**: While initially focused on a few contracts, the design considers future expansion (more contracts, more complex strategies, higher data volumes) by using appropriate technologies like TimescaleDB and modular Python services.
*   **Idempotency**: Operations, especially database writes and state updates, are designed to be safe if re-executed (e.g., due to a service restart). `INSERT ... ON CONFLICT DO NOTHING` is an example.
*   **Logging**: The `analyzer_service.py` and `trend_start_finder.py` include detailed logging for operations, signal detection, and errors. The logs in `trend_start_finder.py` have been refined to be "trader-friendly."

## Development Environment & Project Structure Overview

*   **Primary Language**: Python 3.10+
*   **Key Directories**:
    *   `src/`: Contains all core application code.
        *   `src/core/`: Core utilities, configuration loading (`config.py`).
        *   `src/data/`: Data-related modules.
            *   `src/data/ingestion/`: Live data ingestion (`live_ingester.py`), historical data client (`gateway_client.py`).
            *   `src/data/storage/`: Database interaction logic (`db_handler.py`).
            *   `src/data/models.py`: Pydantic models for data structures like `Bar`.
        *   `src/analysis/`: Signal generation services (`analyzer_service.py`).
        *   `src/strategies/`: Trading strategy implementations (`trend_start_finder.py`).
    *   `scripts/`: Utility scripts for setup, testing, or one-off tasks (`setup_local_db.py`, `download_historical.py`).
    *   `config/`: Configuration files (`settings.yaml`).
    *   `docs/`: Project documentation (like this file).
    *   `tests/`: (Future) Unit and integration tests.
*   **Dependency Management**: `requirements.txt` lists all Python package dependencies.

## Step-by-Step Implementation Plan

A new developer should familiarize themselves with Phase 0 and Phase 1 to understand the data pipeline, then Phase 2 for how signals are generated.

### Phase 0: Prerequisites & Setup

**Goal**: Establish a functional local development environment with all necessary tools and configurations. This phase is foundational for any further development.

- [x] **Environment Setup**:
    *   **Action**: Ensure your Python environment (preferably a virtual environment, e.g., using `venv` or `conda`) has the libraries listed in `requirements.txt`.
    *   **Key Libraries**:
        *   `signalrcore`: For SignalR WebSocket communication (live data).
        *   `psycopg2-binary` (or `asyncpg` for async DB operations): PostgreSQL adapter for Python. `psycopg2-binary` is currently used for synchronous operations in `live_ingester.py` (managed with a thread pool) and `analyzer_service.py` (with `asyncpg` for async operations).
        *   `python-dotenv`: To load environment variables from a `.env` file.
        *   `pyyaml`: To load configurations from YAML files.
        *   `pandas`: Used for data manipulation, especially for handling OHLC dataframes before passing to strategy logic.
- [x] **Configuration Files**:
    *   **`config/settings.yaml`**: The primary configuration file. Defines parameters like API endpoints, contracts to trade, timeframes for OHLC aggregation and analysis, database connection details (template), and strategy-specific parameters.
        *   *Note: `settings.yaml` uses a DSN template for database connections, with the password typically sourced from an environment variable for security.*
    *   **`.env` File (Create locally, not committed to Git)**: Stores sensitive credentials. Copy `env.template` to `.env` and fill in your actual credentials.
        *   `LOCAL_DB_PASSWORD`: Password for your local TimescaleDB instance.
        *   `PROJECTX_API_TOKEN`: Your ProjectX Gateway API token for live data.
        *   `PROJECTX_USERNAME`: Username associated with the API token (used for historical data client).
- [x] **TimescaleDB (Local Setup using Docker)**:
    *   **Action**: Run the `run_timescaledb_docker.sh` script. This script pulls the TimescaleDB Docker image and starts a local container instance.
    *   **Verification**: Connect to this local instance using a database management tool (e.g., DBeaver, pgAdmin, `psql` CLI). Connection details are typically `host=localhost`, `port=5432`, `user=postgres`, `password` (from your `.env` or the default in `run_timescaledb_docker.sh` if not overridden). The default database is `projectx_db`.
- [x] **TimescaleDB Schema (Local Database Initialization)**:
    *   **Action**: Run the `scripts/setup_local_db.py` script. This script connects to the local TimescaleDB and creates necessary tables:
        *   `ohlc_bars`: The main table for storing OHLCV data. It's converted into a TimescaleDB hypertable, partitioned by `timestamp`, for efficient time-series data management. A unique constraint ensures data integrity.
        *   `analyzer_watermarks`: Stores the last processed timestamp for each analyzer instance (combination of analyzer ID, contract, and timeframe). This prevents redundant data processing.
        *   `coordinator_watermarks`: (Future use) Intended to store the last processed signal ID for the signal coordination service.
        *   `detected_signals`: Stores the signals generated by the analyzers.
    *   *The schema definitions in `setup_local_db.py` are the source of truth for the local database structure.*
- [~] **Watermark Tables Schema (Local)**: This item is effectively covered by the `setup_local_db.py` script which creates these tables. Initial population or active use by all services is incremental.
    *   *`analyzer_watermarks` is actively used by `analyzer_service.py`. `coordinator_watermarks` is not yet in use.*

### Phase 1: Data Foundation - Live Data to TimescaleDB

**Goal**: Implement a robust pipeline to ingest live market data, convert it into OHLC bars for multiple timeframes, and store these bars in TimescaleDB. This phase ensures a reliable stream of structured market data for subsequent analysis.

1.  **TimescaleDB Setup**: Covered in Phase 0.

2.  **Live Data Ingestion & OHLC Aggregation Service (`src/data/ingestion/live_ingester.py`)**:
    *   **Purpose**: This is the entry point for live market data into the system.
    *   [x] **Base WebSocket Client (`live_ingester.py`)**:
        *   Connects to the ProjectX Gateway's SignalR hub for market data.
        *   Handles authentication by generating a JWT (using helper script logic, see `generate_jwt_token.py` for standalone generation principle) and including it in the connection.
        *   Subscribes to relevant data streams (quotes and trades) for contracts specified in `config/settings.yaml`.
    *   [x] **Configuration Loading (`src/core/config.py`)**: The `live_ingester.py` uses the `Config` class to load necessary settings (contracts, timeframes, API details) from `config/settings.yaml`.
    *   [x] **In-Memory Bar Aggregator Class (`OHLCBarAggregator` within `live_ingester.py`)**:
        *   **Core Logic**: This class is crucial for transforming high-frequency tick data into structured OHLC bars.
        *   For each (contract, timeframe) pair defined in the configuration:
            *   It maintains a "pending" bar (the current, incomplete bar).
            *   When a new trade (tick) arrives, it updates the `high` (if tick price > current high), `low` (if tick price < current low), `close` (to tick price), and accumulates `volume`. The `open` price is set by the first tick of the bar.
            *   It continuously checks if the current tick's timestamp signifies the completion of a bar period for any of the configured timeframes (e.g., a 1-minute bar completes at the end of each minute).
            *   Completed bars are timestamped to the start of their interval (e.g., a bar from 10:00:00 to 10:00:59 is timestamped 10:00:00).
        *   A callback mechanism (`on_bar_completed`) is used to pass fully formed bars to the database storage logic.
    *   [x] **Database Interaction (Direct `psycopg2` in `live_ingester.py`)**:
        *   Completed OHLC bars are passed to a function that inserts them into the `ohlc_bars` table in TimescaleDB.
        *   To avoid blocking the asynchronous SignalR event loop (which handles incoming ticks), database write operations are executed in a separate thread pool (using `concurrent.futures.ThreadPoolExecutor`). This is a common pattern when integrating synchronous blocking libraries (like `psycopg2`) with an `asyncio` application.
        *   Uses an `INSERT ... ON CONFLICT (contract_id, timestamp, timeframe_unit, timeframe_value) DO NOTHING` statement. This `UPSERT` logic is vital for data integrity, preventing duplicate bar entries if the service restarts or if there's any overlap in data processing.
    *   [x] **Main Ingestion Loop (`live_ingester.py`)**:
        *   The script runs an `asyncio` event loop.
        *   It establishes and maintains the SignalR WebSocket connection.
        *   Upon receiving trade/quote messages, it parses them and passes the relevant information (price, volume, timestamp) to the `OHLCBarAggregator`.
    *   [x] **Asynchronous Operations**: The SignalR client (`signalrcore`) operates asynchronously. Database writes, though using the synchronous `psycopg2` library, are offloaded to a thread pool to prevent them from blocking the main async loop.
    *   [x] **Error Handling & Reconnection**:
        *   The `signalrcore` client has built-in mechanisms for attempting to reconnect if the WebSocket connection drops.
        *   The script includes `try-except` blocks for parsing incoming messages and for database operations, logging any errors encountered.
    *   [x] **Logging**: Structured logging (Python's `logging` module) is used throughout the service to record its operations, any errors, and key events (e.g., connection established, bar completed).
    *   [x] **Historical Data Ingestion (`scripts/download_historical.py`)**:
        *   **Purpose**: To backfill historical OHLC data, which is essential for testing strategies and providing context for live analysis.
        *   **Functionality**: Uses `src/data/ingestion/gateway_client.py` to fetch historical bar data from the ProjectX API. Stores data in the `ohlc_bars` table, adhering to the same schema and `ON CONFLICT DO NOTHING` logic as the live ingester. This ensures consistency between live and historical data.
        *   This script is typically run manually or as a scheduled job to populate the database.

### Phase 2: Multi-Timeframe Signal Generation

**Goal**: Develop services that analyze the stored OHLC data across different timeframes to generate trading signals based on the implemented strategy (CUS/CDS).

3.  **Timeframe-Specific Analyzer Service (`src/analysis/analyzer_service.py`)**:
    *   [x] **Analyzer Structure**: The `analyzer_service.py` acts as an orchestrator. It runs in a loop, and for each "target" (a specific contract and timeframe combination defined in `config/settings.yaml`), it performs an analysis cycle. Currently, it's a single process that iterates through targets.
    *   [x] **Analyzer Configuration**: Loads analysis targets and the `analyzer_id` from `config/settings.yaml` via `src/core/config.py`. The `analyzer_id` helps distinguish signals or watermark entries if multiple analysis strategies were to be run.
    *   [x] **Data Fetching & Watermarking (Polling Method)**:
        *   For each target, the service first queries the `analyzer_watermarks` table to get the `last_processed_timestamp` for that specific analyzer, contract, and timeframe.
        *   It then queries the `ohlc_bars` table for any new bars that have arrived since this last processed timestamp.
        *   This watermarking mechanism is crucial for efficiency, ensuring that data is processed only once.
    *   [x] **Event-Driven Analyzer Implemented**: The `analyzer_service.py` is updated to use PostgreSQL LISTEN/NOTIFY. It listens for new bar notifications, fetches relevant context, and triggers analysis via `trend_start_finder.py` in near real-time.
    *   [x] **Signal Logic Integration (`src/strategies/trend_start_finder.py`)**:
        *   The new OHLC bars (as a Pandas DataFrame) are passed to the `generate_trend_starts` function in `trend_start_finder.py`.
        *   This function contains the CUS/CDS trend-finding logic, adapted from the original `trend_analyzer_alt.py` script. It iterates through the bars, maintains state, and identifies trend start signals.
    *   [x] **Signal Storage Schema (`detected_signals` table)**:
        *   The `detected_signals` table schema includes: `signal_id` (auto-incrementing primary key), `timestamp` (when the signal occurred), `contract_id`, `timeframe`, `signal_type` (e.g., "uptrend_start", "downtrend_start"), `signal_price`, and a `details` JSONB field to store strategy-specific information (like the rule that triggered the signal).
        *   This table was created by `scripts/setup_local_db.py`.
    *   [x] **Signal Publication**: The `analyzer_service.py` takes the signals returned by `generate_trend_starts` and inserts them into the `detected_signals` table.
        *   The service now uses `asyncpg` for database operations to align with its asynchronous nature.
    *   [x] **Logging**: The `analyzer_service.py` and `trend_start_finder.py` include detailed logging for operations, signal detection, and errors. The logs in `trend_start_finder.py` have been refined to be "trader-friendly."

4.  **Trend Start Data API Development (`web/src/app/api/trend-starts/route.ts`)**:
    *   [ ] **API Endpoint Definition**: Define and implement a GET endpoint (e.g., `/api/trend-starts`).
        *   `Supported Query Parameters: timeframe (required, string), contract_id (optional, string), signal_type (optional, string, e.g., "uptrend_start", "downtrend_start"), start_date (optional, ISO 8601 string), end_date (optional, ISO 8601 string), limit (optional, int, default: 100), offset (optional, int, default: 0).`
        *   `Response: JSON array of trend start signals matching the query, sourced from the detected_signals table.`
    *   [ ] **Request Validation**: Implement validation for all incoming query parameters (e.g., correct timeframe format, date format, numeric types for limit/offset).
    *   [ ] **Database Interaction**: Use Prisma client (expected to be available in the `web/` Next.js environment) to connect to TimescaleDB and query the `detected_signals` table.
    *   [ ] **Dynamic Query Construction**: Build the Prisma query dynamically based on the provided and validated query parameters. Handle optional parameters correctly.
    *   [ ] **Response Formatting**: Ensure a consistent JSON response structure. Return appropriate HTTP status codes (e.g., 200 OK, 400 Bad Request for invalid parameters, 500 Internal Server Error).
    *   [ ] **Error Handling**: Implement robust error handling for database connection issues, query execution errors, and unexpected issues.
    *   [ ] **Logging**: Add logging for incoming API requests (including parameters), significant processing steps, and any errors encountered.
    *   [ ] **Documentation (Initial)**: Briefly document the API endpoint, its parameters, and expected response within the project (e.g., in this framework document or a separate API documentation file). Consider generating OpenAPI/Swagger documentation later.

5.  **Signal Coordination Service (Python) (Future Development)**:
    *   **Purpose**: This service will be responsible for taking the raw signals generated by individual analyzers and applying higher-level logic to decide if a trade should actually be considered.
    *   [ ] **Coordinator Structure**: Likely an asynchronous script/service similar to `analyzer_service.py`.
    *   [ ] **Signal Monitoring & Watermarking**: It will query the `detected_signals` table for new signals, using the `coordinator_watermarks` table (specifically `last_processed_signal_id`) to track which signals it has already seen.
    *   [ ] **Initial Coordination Logic (Example)**: A very simple starting point could be to look for agreement across timeframes. For instance, if a "1h uptrend_start" signal appears for "CON.F.US.MES.M25", and shortly after, a "15m uptrend_start" signal also appears for the same contract, the coordinator might log this as a "Coordinated Uptrend Signal."
    *   [ ] **State Management (Basic)**: Might need to keep track of recent signals or the current "coordinated view" for each contract.
    *   [ ] **Output Logging**: Will log its decisions and the reasoning behind them.
    *   [ ] **(Future) Advanced Coordination**: More complex rules could involve:
        *   **Signal Strength/Confidence**: Weighting signals based on certain criteria.
        *   **Strategic vs. Tactical Filters**: Using longer-term signals (e.g., daily) as a strategic filter for shorter-term tactical signals (e.g., 5m, 15m).
        *   **Inter-Strategy Coordination**: If multiple different analysis strategies are running.

### Phase 3: Order Execution Service (Proposed)

This phase focuses on translating coordinated signals into actionable trading orders and managing their lifecycle.

*   **Components**:
    *   `src/execution/execution_service.py`: The main service that listens for coordinated signals (or polls a `coordinated_signals` table), decides on trade actions, and manages order lifecycle.
    *   `src/execution/order_manager.py`: Contains the logic for order generation (e.g., type of order, quantity, stop-loss, take-profit based on rules and risk parameters) based on coordinated signals and current positions.
    *   `src/execution/execution_client.py`: An interface and implementations for connecting to a trading gateway/broker API. Initially, this will be a mock client for simulation.
    *   `src/data/models.py` (or new `src/core/models.py`): Will include an `Order` Pydantic model.
    *   `orders` table (PostgreSQL/TimescaleDB): Stores details of all placed orders (e.g., `order_id`, `contract_id`, `timestamp`, `order_type`, `direction`, `quantity`, `price`, `status`, `stop_loss_price`, `take_profit_price`).
*   **Workflow**:
    1.  The `CoordinatorService` (Phase 2) identifies and potentially stores coordinated signals in a `coordinated_signals` table.
    2.  The `ExecutionService` polls this table (or receives signals via another mechanism like a message queue in a more advanced setup).
    3.  For new coordinated signals, the `OrderManager` is invoked.
    4.  The `OrderManager` checks current positions, risk parameters (from `config/settings.yaml`), and the nature of the signal to decide if an order should be placed.
    5.  If an order is to be placed, the `OrderManager` constructs an `Order` object.
    6.  The `ExecutionClient` is used to "send" the order to the (mock) broker.
    7.  The `ExecutionService` updates the `orders` table with the new order and its initial status (e.g., "pending", "submitted").
    8.  The `ExecutionService` will also need to monitor order statuses (e.g., "filled", "canceled", "rejected") via the `ExecutionClient` and update the `orders` table and potentially a `positions` table.
*   **Configuration (`config/settings.yaml`)**:
    *   `execution:` section:
        *   `execution_service_id`: Identifier for the execution service.
        *   `loop_interval_seconds`: How often the service checks for new signals/updates.
        *   `db_fetch_limit`: Max signals/orders to fetch per cycle.
        *   `provider`: Which execution client to use (e.g., "mock", "tradovate", "interactive_brokers").
        *   `risk_parameters`:
            *   `max_contracts_per_trade`: e.g., 1
            *   `default_stop_loss_pips` (or points/percentage):
            *   `default_take_profit_pips` (or points/percentage):
        *   `accounts`: List of account IDs to trade on.
*   **Key Considerations**:
    *   **Position Management**: Need to track current open positions to avoid duplicate entries or to manage scaling in/out. A `positions` table might be necessary.
    *   **Order Types**: Support for market orders, limit orders, stop orders.
    *   **Risk Management**: Implementing basic risk controls like position sizing, stop-loss, and take-profit levels.
    *   **Error Handling**: Robust handling of API errors, disconnections, rejected orders.
    *   **Idempotency**: Ensuring that the same signal doesn't lead to multiple identical orders.
    *   **Backtesting Integration**: How this execution logic will eventually tie into a backtesting framework.

### Phase 4: Portfolio Management & Advanced Features (Future)

This phase focuses on enhancing the system with additional features and capabilities.

*   **Components**:
    *   `src/portfolio/portfolio_service.py`: This service will manage the overall portfolio of trades and positions.
    *   `src/risk/risk_service.py`: This service will handle risk management and compliance checks.
    *   `src/analytics/analytics_service.py`: This service will provide advanced analytics and reporting capabilities.
*   **Key Features**:
    *   **Portfolio Management**: The `portfolio_service.py` will manage the overall portfolio of trades and positions.
    *   **Risk Management**: The `risk_service.py` will handle risk management and compliance checks.
    *   **Analytics**: The `analytics_service.py` will provide advanced analytics and reporting capabilities.

### General & Maintenance

These are ongoing tasks crucial for the health and evolution of the project.

- [ ] **Unit Tests**:
    *   **Purpose**: To test individual functions and classes in isolation.
    *   **Scope**: Critical components like the `OHLCBarAggregator`, specific strategy rule functions in `trend_start_finder.py`, and utility functions.
    *   **Framework**: Likely `pytest`.
- [ ] **Integration Tests (Basic)**:
    *   **Purpose**: To test the interaction between different components.
    *   **Scope**:
        *   Test the flow from a (mocked) live tick through the `live_ingester.py` to a bar being stored in the database.
        *   Test the flow from OHLC bars in the database through the `analyzer_service.py` to a signal being stored.
- [ ] **Documentation**:
    *   Keep this `mechanical_trading_system_framework.md` document up-to-date with design changes and progress.
    *   Add code comments and docstrings, especially for complex logic or public APIs of classes/modules.

## Deployment to Railway (PostgreSQL/TimescaleDB)

**Goal**: Deploy the developed services (data ingestion, analysis) to a cloud platform (Railway) for continuous operation. Railway is chosen for its ease of use for deploying Dockerized applications and managed PostgreSQL services.

Deploying this system to Railway involves:
1.  Setting up a PostgreSQL database on Railway and enabling the TimescaleDB extension.
2.  Deploying the Python application(s) (Ingestion Service, Analyzer/Coordinator Service) as Docker containers.

### 1. Database Setup on Railway

-   [ ] **Provision PostgreSQL**: In your Railway project, add a new PostgreSQL service. Railway provides a managed database instance.
-   [ ] **Enable TimescaleDB Extension**:
    *   Connect to your Railway PostgreSQL instance. Connection details (host, port, user, password, database name) are available in the Railway service dashboard. You can use a local DB tool (like DBeaver) or `psql` via a secure proxy if Railway provides one.
    *   Execute the SQL command: `CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;`
    *   Verify with `\dx` in `psql`, which should list `timescaledb`.
-   [ ] **Apply Schema**: After enabling the extension, run your schema creation scripts (similar to `scripts/setup_local_db.py`, but adapted to target the Railway DB URL) or manually apply the SQL DDL to create `ohlc_bars` (as a hypertable with unique constraint), `detected_signals`, `analyzer_watermarks`, and `coordinator_watermarks` tables.
-   [ ] **Database URL**: Railway will provide a `DATABASE_URL` (e.g., `postgresql://user:password@host:port/database`). This URL is crucial for your Python application to connect to the database.
-   [ ] **Security**: Use the strong, unique credentials generated by Railway. Review Railway's network access controls if you need to restrict access to the database from outside Railway's internal network.

### 2. Python Application Deployment on Railway

Railway excels at deploying applications from a GitHub repository, especially if a `Dockerfile` is present.

-   [ ] **Create `Dockerfile`**: A `Dockerfile` in your project root tells Railway how to build and run your application.

    ```dockerfile
    # Dockerfile Example for ProjectX Trading System (adjust as needed)
    FROM python:3.10-slim

    WORKDIR /app

    # Install system dependencies if required by Python packages (e.g., libpq-dev for psycopg2 from source, though psycopg2-binary often avoids this)
    # RUN apt-get update && apt-get install -y libpq-dev gcc

    # Copy only requirements.txt first to leverage Docker layer caching
    COPY requirements.txt .
    # Install Python dependencies
    RUN pip install --no-cache-dir -r requirements.txt

    # Copy the rest of the application code
    COPY . .

    # Define the command to run your application.
    # This will depend on how you structure your services.
    # If running analyzer_service.py (which might also call ingestion or they are separate):
    CMD ["python", "-m", "src.analysis.analyzer_service"]
    # Or, if you have a main script that orchestrates multiple services:
    # CMD ["python", "src/main_trader_services.py"]
    # Ensure the entry point script is correctly specified.
    # The `-m` flag is useful if your entry point is a module within a package.
    ```

-   [ ] **Update `requirements.txt`**: Ensure this file is comprehensive and lists all packages needed for the application to run in the Docker container (e.g., `signalrcore`, `psycopg2-binary` or `asyncpg`, `python-dotenv`, `PyYAML`, `pandas`).
-   [ ] **Railway Service Configuration**:
    *   In your Railway project dashboard, add a new service, linking it to your GitHub repository.
    *   Railway should automatically detect the `Dockerfile` and use it for builds.
    *   **Environment Variables**: This is critical for Railway deployments. Configure all necessary environment variables in the Railway service settings (under "Variables"). These will override or provide values that might otherwise come from a local `.env` file:
        *   `DATABASE_URL`: The connection string for your Railway PostgreSQL service.
        *   `PROJECTX_API_TOKEN`: Your ProjectX API token.
        *   `PROJECTX_USERNAME`: Username for token generation/historical data.
        *   Any other settings from `config/settings.yaml` that you wish to manage via environment variables on Railway (e.g., `LOG_LEVEL`).
    *   **Start Command**: Railway will typically use the `CMD` instruction from your `Dockerfile`. You can override this in Railway's service settings ("Deploy" tab) if necessary.
-   [ ] **Service Structure (Consideration for Railway Deployment)**:
    *   **Monolith (Simpler Start)**: Run the Data Ingestion (`live_ingester.py`) and Analysis (`analyzer_service.py`) logic within a single Railway service (possibly using `asyncio.gather` if both are async, or separate threads if one is blocking and needs to run continuously alongside the other). This simplifies initial deployment and management.
    *   **Microservices (More Scalable/Complex but potentially overkill for now)**: Deploy Data Ingestion as one Railway service and Analyzers/Coordinator as another. This provides better resource isolation and independent scaling but adds complexity in terms of inter-service communication (e.g., if they need to signal each other beyond just using the shared database). For now, a monolithic approach is likely sufficient.
-   [ ] **Historical Data Population Strategy for Railway**:
    *   **Option 1 (Run Locally Targeting Railway)**: Modify `scripts/download_historical.py` to accept the Railway `DATABASE_URL` as an environment variable or argument and run it from your local machine to populate the Railway database. This is often the simplest for a one-time backfill.
    *   **Option 2 (One-off Job on Railway)**: If Railway supports one-off jobs or tasks, adapt `download_historical.py` to run as such.
    *   **Option 3 (Initial Fetch in Live Service)**: Add logic to `live_ingester.py` or `analyzer_service.py` to perform a limited historical data fetch on its very first startup if the database is empty. This is more complex to manage.

### 3. Logging and Monitoring on Railway

-   [ ] **Standard Output Logging**: Ensure your Python application's `logging` configuration directs logs to `stdout` and `stderr`. Railway automatically captures these streams and displays them in the service logs dashboard.
-   [ ] **Railway Metrics**: Railway provides built-in metrics (CPU usage, memory usage, network I/O, disk usage) for deployed services. Monitor these to ensure your application is running healthily and to determine if resource allocations need adjustment.

### 4. Data Retrieval from Railway PostgreSQL/TimescaleDB

Once data is flowing into your Railway-hosted TimescaleDB:

-   **From your local machine (for analysis, debugging, or ad-hoc queries)**:
    *   Use the PostgreSQL connection string provided by Railway. You can plug this into local database tools (DBeaver, pgAdmin) or use it in local Python scripts with `psycopg2` or `asyncpg`.
    *   Railway might require you to expose your database publicly or connect via a secure proxy/bastion if you're accessing it from outside their network. Check their documentation for current best practices on external database access.
-   **From other Railway services**: If you deploy other applications (e.g., a web dashboard) within the same Railway project, they can typically connect to the PostgreSQL service using its internal Railway network address (often referenced by the service name) and the `DATABASE_URL` environment variable.
-   **Example Python snippet for connection (ensure `DATABASE_URL` is set in the environment)**:

    ```python
    import psycopg2
    import os

    # DATABASE_URL should be available as an environment variable in Railway
    db_url = os.environ.get('DATABASE_URL')

    if not db_url:
        print("Error: DATABASE_URL environment variable not set.")
    else:
        try:
            conn = psycopg2.connect(db_url)
            cur = conn.cursor()
            
            # Example query: Fetch the 10 most recent OHLC bars for a specific contract
            cur.execute("""
                SELECT contract_id, timestamp, open, high, low, close, volume 
                FROM ohlc_bars 
                WHERE contract_id = %s 
                ORDER BY timestamp DESC 
                LIMIT 10;
            """, ('CON.F.US.MES.M25',)) # Example contract_id
            
            rows = cur.fetchall()
            for row in rows:
                print(row)
                
            cur.close()
        except (Exception, psycopg2.Error) as error:
            print(f"Error while connecting to PostgreSQL or executing query: {error}")
        finally:
            if 'conn' in locals() and conn:
                conn.close()
    ```

### Checklist for Railway Deployment

This checklist helps ensure all steps are covered when moving to Railway.

- [ ] **Database**: Railway PostgreSQL service created.
- [ ] **Database**: TimescaleDB extension successfully enabled on the Railway PostgreSQL instance.
- [ ] **Database**: All required tables (`ohlc_bars`, `detected_signals`, `analyzer_watermarks`, `coordinator_watermarks`) created in the Railway DB. `ohlc_bars` is confirmed as a hypertable with the correct unique constraint.
- [ ] **Application**: `Dockerfile` is complete, tested locally (e.g., `docker build .`), and correctly defines dependencies and the start command.
- [ ] **Application**: `requirements.txt` accurately lists all runtime dependencies.
- [ ] **Application**: Python service(s) (e.g., combined ingestion/analysis) successfully deployed to Railway from the GitHub repository.
- [ ] **Application**: All necessary environment variables (especially `DATABASE_URL`, `PROJECTX_API_TOKEN`, `PROJECTX_USERNAME`) are correctly configured in the Railway service settings.
- [ ] **Application**: Deployed services on Railway are running, and their logs (viewable in the Railway dashboard) indicate successful startup, connection to the database, and ongoing operations.
- [ ] **Connectivity**: The Python application running on Railway can successfully connect to, write to, and read from the Railway-hosted TimescaleDB instance.
- [ ] **Data Flow**: The end-to-end data flow (live data ingestion, OHLC aggregation, storage, signal generation, signal storage) is functional on Railway. This can be verified by querying the `ohlc_bars` and `detected_signals` tables in the Railway database.

## Goals for Phase Completion

These goals define the expected outcomes and verifiable milestones for each phase of development. A new developer should aim to understand how these goals are met by the existing codebase for completed phases.

### Goals for Phase 0: Prerequisites & Setup

*   [x] **Environment Ready**: Local Python development environment is fully configured with all necessary libraries from `requirements.txt`.
*   [x] **Configuration Managed**: `config/settings.yaml` and a local `.env` file (from `env.template`) are correctly set up with necessary configurations and credentials for local development.
*   [x] **Local Database Operational**: Local TimescaleDB instance is running via Docker (using `run_timescaledb_docker.sh`), accessible with a DB tool, and the `timescaledb` extension is enabled.
*   [x] **Local Database Schema Applied**: All required tables (`ohlc_bars`, `analyzer_watermarks`, `coordinator_watermarks`, `detected_signals`) are created in the local TimescaleDB by `scripts/setup_local_db.py`. `ohlc_bars` is a hypertable, and appropriate unique constraints are applied.
*   [x] **Basic DB Connectivity**: Python scripts (`setup_local_db.py`, `live_ingester.py`, `analyzer_service.py`) can successfully connect to the local TimescaleDB instance.

### Goals for Phase 1: Data Foundation - Live Data to TimescaleDB

*   [x] **WebSocket Connection Established**: The Live Data Ingestion service (`live_ingester.py`) can successfully authenticate and connect to the SignalR WebSocket for market data.
*   [x] **Live Data Reception**: The service receives and parses live trade/quote data for all subscribed contracts.
*   [x] **Accurate OHLC Aggregation**: The `OHLCBarAggregator` (in `live_ingester.py`) correctly processes incoming trades and forms accurate OHLC bars for all configured (contract, timeframe) pairs, with proper timestamp alignment to standard clock intervals.
*   [x] **Reliable Bar Storage**: Completed OHLC bars are consistently and correctly stored in the local TimescaleDB `ohlc_bars` table with `ON CONFLICT DO NOTHING` logic.
*   [x] **Duplicate Prevention Verified**: The `ON CONFLICT DO NOTHING` logic for `ohlc_bars` effectively prevents duplicate bar entries.
*   [x] **Service Resilience**: The `live_ingester.py` service demonstrates error handling for WebSocket communication (reconnection) and data processing, with adequate logging.
*   [x] **Sustained Operation (Local)**: The service can run for an extended period locally, continuously ingesting and storing live data.
*   [x] **Historical Data Backfill**: `scripts/download_historical.py` can successfully fetch and store historical OHLC data into the `ohlc_bars` table, consistent with the live data schema.

### Goals for Phase 2: Multi-Timeframe Signal Generation

*   [x] **Analyzer Data Retrieval**: The `analyzer_service.py` successfully queries new OHLC bars from TimescaleDB for each configured target (contract, timeframe), using the `analyzer_watermarks` table to avoid reprocessing data (current polling method).
*   [x] **Event-Driven Analyzer Implemented**: The `analyzer_service.py` is updated to use PostgreSQL LISTEN/NOTIFY. It listens for new bar notifications, fetches relevant context, and triggers analysis via `trend_start_finder.py` in near real-time.
*   [x] **Strategy Logic Integration**: The core CUS/CDS trend-finding logic from `src/strategies/trend_start_finder.py` is successfully integrated and called by `analyzer_service.py`, processing OHLC data retrieved from the database.
*   [x] **Signal Generation & Storage**: The `analyzer_service.py` correctly identifies trend signals based on the strategy logic and reliably stores these signals (including descriptive rule names) in the `detected_signals` table.
*   [x] **Analyzer Watermark Update**: `analyzer_service.py` correctly updates the `last_processed_timestamp` in the `analyzer_watermarks` table for each target after processing.
*   [x] **Coordinator Signal Retrieval**: The Signal Coordination Service (`src/coordination/coordinator_service.py`) can successfully query new signals from the `detected_signals` table, using the `coordinator_watermarks` table to track its progress.
*   [x] **Basic Coordination Logic Functional**: The coordinator implements initial, rule-based coordination (e.g., time-based confluence of signals from different timeframes for the same contract and trend type) and logs coordinated signal events. A signal cache is used to manage recent signals for evaluation.
*   [x] **Coordinator Watermark Update**: The coordinator correctly updates its `last_processed_signal_id` in the `coordinator_watermarks` table after each processing cycle.
*   [x] **End-to-End Data Flow (Signal Generation & Basic Coordination)**: A clear data path is observable and functional: Live Tick -> OHLC Bar in DB -> Analyzer Reads Bar -> Analyzer Writes Signal to DB -> Coordinator Reads Signal -> Coordinator Logs Coordinated Event.

### Goals for Phase 3: Order Execution Service (Proposed)

*   [ ] **Execution Module Design**: A clear design document or detailed specification for the Order Execution Module is created, outlining its responsibilities, inputs (e.g., coordinated signals), outputs (e.g., order submissions), and interaction with the `GatewayClient` and Signal Coordination Service.
*   [ ] **API Interaction Plan**: Key ProjectX Gateway API endpoints for order placement, modification, cancellation, and position/account status retrieval are identified, and their usage within the execution module is planned.
*   [ ] **Risk Management Considerations**: Initial thoughts on how basic risk checks (e.g., max position size per trade, daily loss limits, concurrent open orders) would be incorporated before order placement are documented.

### Goals for Railway Deployment (Post-Local Development & Testing)

*   [ ] **Railway Database Setup**: Railway PostgreSQL service is provisioned, TimescaleDB extension is enabled, and all necessary schemas (`ohlc_bars`, `detected_signals`, watermark tables) are applied correctly and verified.
*   [ ] **Application Dockerized & Deployed**: Python application(s) are successfully containerized using the `Dockerfile` and deployed as a service on Railway, with correct start commands.
*   [ ] **Environment Configuration on Railway**: All necessary environment variables (DATABASE_URL, API keys, service-specific configurations) are securely configured in the Railway service settings.
*   [ ] **Service Operational on Railway**: Deployed services on Railway are running, connecting to the Railway DB, and processing live data (verifiable through Railway logs and by querying the Railway DB). This includes data ingestion and signal generation.
*   [ ] **Remote Data Accessibility**: OHLC and signal data stored in the Railway TimescaleDB instance can be accessed remotely (e.g., from a local DB tool or script) for verification, analysis, and debugging, respecting Railway's security configurations.
*   [ ] **Historical Data Population on Railway**: A strategy for populating the Railway DB with historical data is chosen and successfully executed.

</rewritten_file> 